{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exam1",
      "provenance": [],
      "authorship_tag": "ABX9TyNcE7+q1C8UcttrRQ9uQbMN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nogbazghi/CSC8980/blob/main/Exam1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owTkTYjtlzjt"
      },
      "source": [
        "Nahom Ogbazghi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeyiwSGRcEr5"
      },
      "source": [
        "002052292"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s80JYwCywP5A"
      },
      "source": [
        "\r\n",
        "!pip install wget\r\n",
        "import wget\r\n",
        "\r\n",
        "print('Downloading dataset...')\r\n",
        "!unzip exam1_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnyjA4B0A86l"
      },
      "source": [
        "import os\r\n",
        "from sklearn.datasets import load_files\r\n",
        "\r\n",
        "#I did not see a random seed but a random state\r\n",
        "random_state = 12345\r\n",
        "\r\n",
        "data = load_files('exam1_dataset/TRAINING', random_state=random_state)\r\n",
        "categories = data.target_names\r\n",
        "\r\n",
        "class Pairs:\r\n",
        "  def __init__(self, data, target):\r\n",
        "        self.data = data\r\n",
        "        self.target = target"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd16IgqXl2q5"
      },
      "source": [
        "Question 1) (20 points) Write a generic function that takes: Classification algorithm name,\r\n",
        "vectorization method name, training set with labels as parameters (total of 3 parameters should\r\n",
        "be passed). The function should take the classification algorithm name, the vectorization\r\n",
        "method’s name, and the training set and train the desired model. Use the default training\r\n",
        "parameters for the models we have seen in class. This function should return the trained model.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZbKoytylwCk"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\r\n",
        "\r\n",
        "def trainModel(classificationAlgorithmName, vectorizationMethodName, train):\r\n",
        "  model = make_pipeline(vectorizationMethodName, classificationAlgorithmName)\r\n",
        "  model.fit(train.data, train.target)\r\n",
        "  return model\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_REpEShPpOpT"
      },
      "source": [
        "Question 2) (30 points) Using the function from question 1 to build the following models:\r\n",
        "a) Model a: Naive Bayes, Vectorizer: TFIDF and Bag of Words, Training set should be 75%\r\n",
        "of the provided dataset. Leaving the remaining 25% for testing.\r\n",
        "b) Model b: RandomForest, Vectorizer: TFIDF and Bag of Words, Training set should be\r\n",
        "70% of the provided dataset. Leaving the remaining 30% for testing.\r\n",
        "c) Model c: Support Vector Machines (SVC in sklearn), Vectorizer: TFIDF and Bag of\r\n",
        "Words, Training set should be 60% of the provided dataset. Leaving the remaining 40%\r\n",
        "for testing.\r\n",
        "NOTE: Set the random seed to: 12345. This needs to be consistently set to train the model\r\n",
        "AND split the data in test and train. If this is not done correctly, you will lose points as your\r\n",
        "answers will not be comparable with the grading key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ1vQW70psvw",
        "outputId": "ea7e6315-111d-415d-baa4-760c1e035247"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.pipeline import make_pipeline\r\n",
        "\r\n",
        "vTFIDF = TfidfVectorizer()\r\n",
        "vBOW = CountVectorizer()\r\n",
        "data_to_split = data.data\r\n",
        "target_to_split = data.target\r\n",
        "\r\n",
        "#I did not see a random seed but a random state\r\n",
        "random_state = 12345\r\n",
        "\r\n",
        "print(\"Naive Bayes Training\")\r\n",
        "nb_training_data, nb_testing_data, nb_training_target, nb_testing_target = train_test_split(data_to_split, target_to_split, train_size=0.75)\r\n",
        "nb_training = Pairs(nb_training_data, nb_training_target)\r\n",
        "print(\"TFIDF\")\r\n",
        "nb_tf_model = trainModel(MultinomialNB(), TfidfVectorizer(), nb_training)\r\n",
        "nb_tf_model.fit(nb_training.data, nb_training.target)\r\n",
        "print(\"BOW\")\r\n",
        "nb_bow_model = trainModel(MultinomialNB(), CountVectorizer(), nb_training)\r\n",
        "nb_bow_model.fit(nb_training.data, nb_training.target)\r\n",
        "\r\n",
        "print(\"Random Forest Training\")\r\n",
        "rf_training_data, rf_testing_data, rf_training_target, rf_testing_target = train_test_split(data_to_split, target_to_split, train_size=0.7)\r\n",
        "rf_training = Pairs(rf_training_data, rf_training_target)\r\n",
        "print(\"TFIDF\")\r\n",
        "rf_tf_model = trainModel(RandomForestClassifier(random_state=random_state), TfidfVectorizer(), rf_training)\r\n",
        "rf_tf_model.fit(rf_training.data, rf_training.target)\r\n",
        "print(\"BOW\")\r\n",
        "rf_bow_model = trainModel(RandomForestClassifier(random_state=random_state), CountVectorizer(), rf_training)\r\n",
        "rf_bow_model.fit(rf_training.data, rf_training.target)\r\n",
        "\r\n",
        "print(\"SVM Training\")\r\n",
        "#I tried using SVC, but it repeatedly crashed my runs so I opted to use LinearSVC\r\n",
        "svm_training_data, svm_testing_data, svm_training_target, svm_testing_target = train_test_split(data_to_split, target_to_split, train_size=0.6, random_state=random_state)\r\n",
        "svm_training = Pairs(svm_training_data, svm_training_target)\r\n",
        "print(\"TFIDF\")\r\n",
        "svm_tf_model = trainModel(LinearSVC(random_state=random_state), TfidfVectorizer(), svm_training)\r\n",
        "svm_tf_model.fit(svm_training.data, svm_training.target)\r\n",
        "print(\"BOW\")\r\n",
        "svm_bow_model = trainModel(LinearSVC(random_state=random_state), CountVectorizer(), svm_training)\r\n",
        "svm_bow_model.fit(svm_training.data, svm_training.target)\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Training\n",
            "TFIDF\n",
            "BOW\n",
            "Random Forest Training\n",
            "TFIDF\n",
            "BOW\n",
            "SVM Training\n",
            "TFIDF\n",
            "BOW\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('countvectorizer',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('linearsvc',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='squared_hinge', max_iter=1000,\n",
              "                           multi_class='ovr', penalty='l2', random_state=12345,\n",
              "                           tol=0.0001, verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BofsVCHXIReC"
      },
      "source": [
        "Question 3) (30 points) Using the models from Question 2, evaluate each model with its\r\n",
        "respective training set (for model a, that set is 25% of the data, for model b, that set is 30% of\r\n",
        "the data, and for model c that set is 40% of the data. Be careful to not mix up the evaluation\r\n",
        "sets. With the predictions on the test set and show the following metrics: Accuracy, Precision,\r\n",
        "Recall, and Macro F1-score. With this in mind, please write and answer these questions in your\r\n",
        "notebook:\r\n",
        "a) What model performs the best and why? (which metrics do you base this on, and why do\r\n",
        "you think it performs better than others).\r\n",
        "b) Why is it important not to mix up the testing sets between different models? Think about\r\n",
        "this one.\r\n",
        "c) Display in a single sorted dataframe (model name, training %, test %, accuracy,\r\n",
        "precision, recall, F1-score) all performance metrics, sorted by accuracy in descending\r\n",
        "manner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaNZMLu4JChj",
        "outputId": "489a9ee5-b6d1-4824-a870-a986e67fb82f"
      },
      "source": [
        "import sklearn.metrics\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "print(\"Naive Baies Prediction\")\r\n",
        "print(\"TFIDF\")\r\n",
        "nb_tf_labels = nb_tf_model.predict(nb_testing_data)\r\n",
        "nb_tfList = [\"NB_TFIDF\", 0.75, 0.25, sklearn.metrics.accuracy_score(nb_testing_target, nb_tf_labels), sklearn.metrics.precision_score(nb_testing_target, nb_tf_labels), sklearn.metrics.recall_score(nb_testing_target, nb_tf_labels), sklearn.metrics.f1_score(nb_tf_labels, nb_testing_target, average='macro')]\r\n",
        "print(\"BOW\")\r\n",
        "nb_bow_labels = nb_bow_model.predict(nb_testing_data)\r\n",
        "nb_bowList = [\"NB_BOW\", 0.75, 0.25, sklearn.metrics.accuracy_score(nb_testing_target, nb_bow_labels), sklearn.metrics.precision_score(nb_testing_target, nb_bow_labels), sklearn.metrics.recall_score(nb_testing_target, nb_bow_labels), sklearn.metrics.f1_score(nb_bow_labels, nb_testing_target, average='macro')]\r\n",
        "\r\n",
        "print(\"Random Forest Predicting\")\r\n",
        "print(\"TFIDF\")\r\n",
        "rf_tf_labels = rf_tf_model.predict(rf_testing_data)\r\n",
        "rf_tfList = [\"RF_TFIDF\", 0.7, 0.3, sklearn.metrics.accuracy_score(rf_testing_target, rf_tf_labels), sklearn.metrics.precision_score(rf_testing_target, rf_tf_labels), sklearn.metrics.recall_score(rf_testing_target, rf_tf_labels), sklearn.metrics.f1_score(rf_tf_labels, rf_testing_target, average='macro')]\r\n",
        "print(\"BOW\")\r\n",
        "rf_bow_labels = rf_bow_model.predict(rf_testing_data)\r\n",
        "rf_bowList = [\"RF_BOW\", 0.7, 0.3, sklearn.metrics.accuracy_score(rf_testing_target, rf_bow_labels), sklearn.metrics.precision_score(rf_testing_target, rf_bow_labels), sklearn.metrics.recall_score(rf_testing_target, rf_bow_labels), sklearn.metrics.f1_score(rf_bow_labels, rf_testing_target, average='macro')]\r\n",
        "\r\n",
        "print(\"SVM Prediction\")\r\n",
        "print(\"TFIDF\")\r\n",
        "svm_tf_labels = svm_tf_model.predict(svm_testing_data)\r\n",
        "svm_tfList = [\"SVM_TFIDF\", 0.6, 0.4, sklearn.metrics.accuracy_score(svm_testing_target, svm_tf_labels), sklearn.metrics.precision_score(svm_testing_target, svm_tf_labels), sklearn.metrics.recall_score(svm_testing_target, svm_tf_labels), sklearn.metrics.f1_score(svm_tf_labels, svm_testing_target, average='macro')]\r\n",
        "print(\"BOW\")\r\n",
        "svm_bow_labels = svm_bow_model.predict(svm_testing_data)\r\n",
        "svm_bowList = [\"SVM_BOW\", 0.6, 0.4, sklearn.metrics.accuracy_score(svm_testing_target, svm_bow_labels), sklearn.metrics.precision_score(svm_testing_target, svm_bow_labels), sklearn.metrics.recall_score(svm_testing_target, svm_bow_labels), sklearn.metrics.f1_score(svm_bow_labels, svm_testing_target, average='macro')]\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Baies Prediction\n",
            "TFIDF\n",
            "BOW\n",
            "Random Forest Predicting\n",
            "TFIDF\n",
            "BOW\n",
            "SVM Prediction\n",
            "TFIDF\n",
            "BOW\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyUuA5i8JBAC"
      },
      "source": [
        "a) What model performs the best and why? (which metrics do you base this on, and why do you think it \r\n",
        "performs better than others). \r\n",
        "\r\n",
        "SVM using TFIDF performed the best. I base this primarily on Recall since it scored the highest in that and it can be double checked for any false positives as it is easy to do so with high recalls. It performed well because it is good for accuracy, but it may overfit.\r\n",
        "\r\n",
        "b) Why is it important not to mix up the testing sets between different models? Think about this one. \r\n",
        "\r\n",
        "It's important not to mix the testing sets between different models because the splits allocate a percentage of the data between a test and train sets and if a model that was originally trained with NB training set(75%), but was then tested with SVMs testing set(40%) there will be an overlap of testing data that is within the trained model. This will return a higher than normal score since the model will essentially be tested on data that it already knows. \r\n",
        "\r\n",
        "c) Display in a single sorted dataframe (model name, training %, test %, accuracy, precision, recall, F1-score) all performance metrics, sorted by accuracy in descending manner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnJ2OZvIfHWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c7d2445-12b7-43eb-8da0-f9ab6eb0027d"
      },
      "source": [
        "listofResults = [nb_tfList, nb_bowList, rf_tfList, rf_bowList, svm_tfList, svm_bowList]\r\n",
        "df = pd.DataFrame(np.array(listofResults), columns=[\"Name\", \"Training %\", \"Test %\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\r\n",
        "sortedDf = df.sort_values(by=\"Accuracy\", ascending=False)\r\n",
        "print(sortedDf)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Name Training %  ...              Recall            F1 Score\n",
            "4  SVM_TFIDF        0.6  ...  0.8926936093967749  0.8894941111411827\n",
            "5    SVM_BOW        0.6  ...  0.8582520406131794   0.854591601210886\n",
            "0   NB_TFIDF       0.75  ...  0.8354964308890331  0.8518838769595363\n",
            "1     NB_BOW       0.75  ...  0.8072680077871512  0.8377949592879452\n",
            "3     RF_BOW        0.7  ...  0.8557227297152069  0.8373099059597915\n",
            "2   RF_TFIDF        0.7  ...  0.8288554540569586  0.8275958040649913\n",
            "\n",
            "[6 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vqOgRUbfGg4"
      },
      "source": [
        "Question 4) (15 points) Using the documents in the folder named UNLABELED, please use\r\n",
        "your best performing trained model from question 3 to predict their labels. Please do this\r\n",
        "individually for each document. Print to the screen the following items: Document Name,\r\n",
        "Predicted Label and using a text cell, write your own opinion if the label is correct and why -\r\n",
        "note you have to read the document to make your own opinion.\r\n",
        "\r\n",
        "**\r\n",
        "\r\n",
        "*   filename: 46278_0.txt | predicted labeled: positive | yes, they were originally look warm but at the end it was positive \r\n",
        "*   filename: 36517_0.txt | predicted labeled: negative | yes, they described it as drespressingly dreary\r\n",
        "*   filename: 24221_0.txt | predicted labeled: negative | they could not recommend it \r\n",
        "*   filename: 46705_0.txt | predicted labeled: negative | yes, they mentioned it only got worse and worse\r\n",
        "*   filename: 37154_0.txt | predicted labeled: negative |  yes, they listed several negatives\r\n",
        "*   filename: 0_0.txt | predicted labeled: positive | I think it read negative, but labeled positive, so not accurate\r\n",
        "*   filename: 36022_0.txt | predicted labeled: negative | yes, they mentioned the creator had better films\r\n",
        "*   filename: 36149_0.txt | predicted labeled: positive | no, the user described it as making them sick \r\n",
        "*   filename: 35991_0.txt | predicted labeled: negative | yes, they said it was a bad film\r\n",
        "*   filename: 49990_0.txt | predicted labeled: negative | yes, used words like sadly nonexistant\r\n",
        "*   filename: 35968_0.txt | predicted labeled: positive | no, I think they described overall negatively\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zXuEjxnfZSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c95cb3f-4304-48dc-d043-3b642b18207f"
      },
      "source": [
        "path = '/content/exam1_dataset/UNLABELED/'\r\n",
        "for filename in os.listdir(path):\r\n",
        "  if filename.endswith(\".txt\"):\r\n",
        "    file = open(path+str(filename), 'r')\r\n",
        "    content = file.read()\r\n",
        "    label = svm_tf_model.predict([content])\r\n",
        "    print(\"filename:\",filename, \"| predicted labeled:\", categories[label[0]])\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['negative', 'positive']\n",
            "filename: 46278_0.txt | predicted labeled: positive\n",
            "filename: 36517_0.txt | predicted labeled: negative\n",
            "filename: 24221_0.txt | predicted labeled: negative\n",
            "filename: 46705_0.txt | predicted labeled: negative\n",
            "filename: 37154_0.txt | predicted labeled: negative\n",
            "filename: 0_0.txt | predicted labeled: positive\n",
            "filename: 36022_0.txt | predicted labeled: negative\n",
            "filename: 36149_0.txt | predicted labeled: positive\n",
            "filename: 35991_0.txt | predicted labeled: negative\n",
            "filename: 49990_0.txt | predicted labeled: negative\n",
            "filename: 35968_0.txt | predicted labeled: positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHoyD_DbfZsx"
      },
      "source": [
        "Question 5) (20 points) Build a function that takes the set of documents as input and returns a\r\n",
        "cosine similarity matrix for those documents. Feed all documents in the TRAINING folder to this\r\n",
        "matrix. Instead of printing the returned cosine similarity matrix, create a heatmap plot from the\r\n",
        "returned matrix. Make sure your plot is nicely scaled, properly labeled, and uses a nice color\r\n",
        "range to show the similarity.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T76_G4DsA92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df101a66-062d-4daf-d033-bcb10c0e8ecb"
      },
      "source": [
        "import nltk\r\n",
        "\r\n",
        "nltk.download('reuters')\r\n",
        "nltk.download('punkt')\r\n",
        "allFilenames = []\r\n",
        "documents = []\r\n",
        "path = '/content/exam1_dataset/TRAINING/'\r\n",
        "for c in categories:\r\n",
        "  for f in os.listdir(path+c+\"/\"):\r\n",
        "    filename = c+\"/\"+str(f)\r\n",
        "    allFilenames.append(str(filename))\r\n",
        "    file = open(path+str(filename), 'r')\r\n",
        "    content = file.read()\r\n",
        "    documents.append((filename, content))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "S_mq-XF0ffxG",
        "outputId": "3add82b5-de49-4fcd-ee56-f1cd15869cd2"
      },
      "source": [
        "import math\r\n",
        "import sklearn.metrics\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "def getMatrix(documents):\r\n",
        "  matrix = {}\r\n",
        "  for doc in documents:\r\n",
        "    filename = doc[0]\r\n",
        "    content = doc[1]\r\n",
        "    content = file.read()\r\n",
        "    tokens = word_tokenize(content)\r\n",
        "    for word in tokens:\r\n",
        "      if word in matrix:\r\n",
        "        wordFilenameCount = matrix[word]\r\n",
        "        if filename in wordFilenameCount: \r\n",
        "          wCcount = wordFilenameCount[filename]\r\n",
        "          wordFilenameCount[filename] = wCcount + 1\r\n",
        "          matrix[word] = wordFilenameCount\r\n",
        "        else:\r\n",
        "          wordFilenameCount[filename] = 1\r\n",
        "          matrix[word] = wordFilenameCount\r\n",
        "      else:\r\n",
        "        insertDictIntoMW={}\r\n",
        "        insertDictIntoMW[filename] = 1\r\n",
        "        matrix[word] = insertDictIntoMW\r\n",
        "  return matrix\r\n",
        "\r\n",
        "\r\n",
        "def find_tf_id(count):\r\n",
        "  return math.log(count+1, 10)\r\n",
        "\r\n",
        "def find_df(matrix, word):\r\n",
        "  docs = matrix[word]\r\n",
        "  return len(docs.keys())\r\n",
        "\r\n",
        "def find_idf(matrix, word):\r\n",
        "  df = find_df(matrix, word)\r\n",
        "  count = len(allFilenames)/df\r\n",
        "  return math.log(count, 10)\r\n",
        "\r\n",
        "def find_tf_idf(tf, idf):\r\n",
        "  return tf * idf\r\n",
        "\r\n",
        "# idf per word\r\n",
        "def get_idf_matrix(words, matrix):\r\n",
        "  imatrix = {}\r\n",
        "  for word in words:\r\n",
        "    idf = find_idf(matrix, word)\r\n",
        "    imatrix[word] = idf\r\n",
        "  return imatrix\r\n",
        "\r\n",
        "# tf idf per word\r\n",
        "def get_tfidf_matrid(matrix, words, idf_matrix):\r\n",
        "  tmatrix = {} #key:filename, value: {key:word, value:tf idf} \r\n",
        "  for word in words:\r\n",
        "    wcPerFileForWord = matrix[word]\r\n",
        "    filenamesOfWord = wcPerFileForWord.keys()\r\n",
        "    # print(filenamesOfWord)\r\n",
        "    for filename in filenamesOfWord:\r\n",
        "      wordCount = wcPerFileForWord[filename]\r\n",
        "      tf = find_tf_id(wordCount)\r\n",
        "      idf = idf_matrix[word]\r\n",
        "      if filename in tmatrix:\r\n",
        "        tfsPerWord = tmatrix[filename]\r\n",
        "      else:\r\n",
        "        tfsPerWord = {}\r\n",
        "      tfsPerWord[word] = find_tf_idf(tf, idf)\r\n",
        "    tmatrix[filename] = tfsPerWord\r\n",
        "  return tmatrix\r\n",
        "\r\n",
        "def cosSimOfTwoDocs(fileA, fileB, tf_idf_matrix):\r\n",
        "  fileAwks = list(tf_idf_matrix[fileA].values())\r\n",
        "  fileBwks = list(tf_idf_matrix[fileB].values())\r\n",
        "  kA = len(fileAwks)\r\n",
        "  kB = len(fileBwks)\r\n",
        "  for tfidfScore in fileAwks:\r\n",
        "    anum = anum + tfidfScore\r\n",
        "  \r\n",
        "  for tfidfScore in fileBwks:\r\n",
        "    bnum = bnum + tfidfScore\r\n",
        "  dA = anum/kB\r\n",
        "  dB = bnum/kB\r\n",
        "  num = dA + dB\r\n",
        "  den = dA * dB\r\n",
        "  return num/den\r\n",
        "\r\n",
        "\r\n",
        "def getcosDocPairs(documents, tfidfMatrix):\r\n",
        "  cosDocPairs = {}\r\n",
        "  for fileA,content in documents:\r\n",
        "    cosSimVals = {}\r\n",
        "    filesinCosDocPairs = cosDocPairs.keys()\r\n",
        "    if fileA in filesinCosDocPairs:\r\n",
        "      cosSimVals = cosDocPairs[fileA]\r\n",
        "    for fileB,content in documents:\r\n",
        "      cosSim = cosSimOfTwoDocs(fileA, fileB, tfidfMatrix)\r\n",
        "      cosSimVals[fileB] = cosSim\r\n",
        "    cosDocPairs[fileA] = cosSimVals\r\n",
        "  return cosDocPairs\r\n",
        "\r\n",
        "matrix = getMatrix(allFilenames)\r\n",
        "words = matrix.keys()\r\n",
        "idf_matrix = get_idf_matrix(words, matrix)\r\n",
        "tf_idf_matrix = get_tfidf_matrid(matrix, words, idf_matrix)\r\n",
        "cosDocPairs = getcosDocPairs(documents, tf_idf_matrix)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bddd36d77a0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0midf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_idf_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mtf_idf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tfidf_matrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mcosDocPairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetcosDocPairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-bddd36d77a0e>\u001b[0m in \u001b[0;36mgetcosDocPairs\u001b[0;34m(documents, tfidfMatrix)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mcosSimVals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosDocPairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfileA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mcosSim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosSimOfTwoDocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidfMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0mcosSimVals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfileB\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosSim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mcosDocPairs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfileA\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosSimVals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-bddd36d77a0e>\u001b[0m in \u001b[0;36mcosSimOfTwoDocs\u001b[0;34m(fileA, fileB, tf_idf_matrix)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosSimOfTwoDocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mfileAwks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfileA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0mfileBwks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfileB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mkA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileAwks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'negative/12268_1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbdxk1gjQqUu"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05I2Zt06RR-Q"
      },
      "source": [
        "Question 6) (15 points) Write a function that takes a cosine similarity matrix as input and\r\n",
        "returns a list with the top n document paris and their similarity. Note that you should only keep\r\n",
        "the document pairs that are unique and remove the comparisons of the document to itself. Print\r\n",
        "the top 50 similar document pairs. Compare the assigned class for each document and answer:\r\n",
        "Do all similar documents belong to the same class? Why or why not?\r\n",
        "\r\n",
        "**\r\n",
        "All assigned classes did belong to the same class, because they used similar words to describe the film. positive reviews used similar positive words, while negative reviews used similar negative words **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w44xJf1SRTnf"
      },
      "source": [
        "def replacedSmallest(m, num, nameA, nameB):\r\n",
        "  values = list(m.values())\r\n",
        "  values.sort()\r\n",
        "  smallest = values[0]\r\n",
        "  if(num > smallest):\r\n",
        "    names = m.keys()\r\n",
        "    for n in names:\r\n",
        "      if m[n] == smallest:\r\n",
        "        m.pop(n)\r\n",
        "        m[(nameA,nameB)] = num\r\n",
        "  return m\r\n",
        "\r\n",
        "def alreadyInList(m, nameA, nameB):\r\n",
        "  names = m.keys()\r\n",
        "  for n in names:\r\n",
        "    if nameA in n:\r\n",
        "      if nameB in n:\r\n",
        "        return True\r\n",
        "  return False\r\n",
        "\r\n",
        "def topNSimilarWorks(cosDocPairs):\r\n",
        "  n = 50\r\n",
        "  nWorks = {}\r\n",
        "  for fA in allFilenames:\r\n",
        "    cosList = cosDocPairs[fA]\r\n",
        "    for fB in allFilenames:\r\n",
        "      if alreadyInList(nWorks, fA, fB) == False: #already seen\r\n",
        "        if fA != fB: # same pair\r\n",
        "          value = cosList[fB]\r\n",
        "          if len(nWorks) == n:\r\n",
        "            nWorks = replacedSmallest(nWorks, value, fA, fB)\r\n",
        "          else:\r\n",
        "            nWorks[(fA,fB)] = value\r\n",
        "  return nWorks.keys()\r\n",
        "\r\n",
        "items = topNSimilarWorks(50)\r\n",
        "print(\"Top\", len(items), \"similar files\")\r\n",
        "for item in items:\r\n",
        "  first = cosDocPairs[item[0]]\r\n",
        "  print ((item[0], item[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJQsiLl48Qn8"
      },
      "source": [
        "Question 7) (20 points) Using Spacy’s part of speech tagger, process all sentences (hint: don’t\r\n",
        "forget to split the reviews) and count how many NOUN and VERB tags are found in all the\r\n",
        "movies review (TRAINING folder) separating them by label. In other words, how many NOUN\r\n",
        "and VERB tags are found in positive reviews, and how many NOUN and VERB tags are found\r\n",
        "in negative reviews. Answer the following question: When comparing both, do you see any\r\n",
        "differences? Why do you think about the differences? Or lack of them.\r\n",
        "\r\n",
        "Positives had more nouns, while negatives had more verbs. I thikn that makes sense since positive expressions will be more descriptive while, negative expressions will be more active. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8m0hEUL8RuU",
        "outputId": "27408cc8-a102-4e35-b94c-af74101397f4"
      },
      "source": [
        "import spacy\r\n",
        "import re\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "\r\n",
        "punctPattern = re.compile(\"[,!?.:;]\")\r\n",
        "\r\n",
        "nounVerbCountByLabel = {}\r\n",
        "nounVerbCountByLabel[\"positive\"] = {\"noun\": 0, \"verb\": 0}\r\n",
        "nounVerbCountByLabel[\"negative\"] = {\"noun\": 0, \"verb\": 0}\r\n",
        "nounVerbCountByLabel[\"punctuation\"] = 0\r\n",
        "count = 0\r\n",
        "regexPunctCount = 0\r\n",
        "base = \"/content/exam1_dataset/TRAINING/\"\r\n",
        "\r\n",
        "for filename in allFilenames:\r\n",
        "  if filename.endswith(\".txt\"):\r\n",
        "    file = open(base+str(filename), 'r')\r\n",
        "    content = file.read()\r\n",
        "    # QUESTION 8\r\n",
        "    punctMatchCount = re.findall(punctPattern, content)\r\n",
        "    regexPunctCount = regexPunctCount + len(punctMatchCount)\r\n",
        "\r\n",
        "    doc = nlp(content)\r\n",
        "    for t in doc:\r\n",
        "      POS = (spacy.explain(t.pos_)).lower()\r\n",
        "      if \"noun\" in POS:\r\n",
        "        if \"positive\" in filename:\r\n",
        "          positiveCount = nounVerbCountByLabel[\"positive\"]\r\n",
        "          nounCount = positiveCount[\"noun\"]\r\n",
        "          positiveCount[\"noun\"] =nounCount+1\r\n",
        "          nounVerbCountByLabel[\"positive\"] = positiveCount\r\n",
        "        else:\r\n",
        "          negativeCount = nounVerbCountByLabel[\"negative\"]\r\n",
        "          nounCount = negativeCount[\"noun\"]\r\n",
        "          negativeCount[\"noun\"] =nounCount+1\r\n",
        "          nounVerbCountByLabel[\"negative\"] = negativeCount\r\n",
        "      elif \"verb\" in POS:\r\n",
        "        if \"positive\" in filename:\r\n",
        "          positiveCount = nounVerbCountByLabel[\"positive\"]\r\n",
        "          verbCount = positiveCount[\"verb\"]\r\n",
        "          positiveCount[\"verb\"] = verbCount + 1\r\n",
        "          nounVerbCountByLabel[\"positive\"] = positiveCount\r\n",
        "        else:\r\n",
        "          negativeCount = nounVerbCountByLabel[\"negative\"]\r\n",
        "          verbCount = negativeCount[\"verb\"]\r\n",
        "          negativeCount[\"verb\"] = verbCount +1\r\n",
        "          nounVerbCountByLabel[\"negative\"] = negativeCount\r\n",
        "      elif \"punctuation\" in POS:\r\n",
        "        punctCount = nounVerbCountByLabel[\"punctuation\"]\r\n",
        "        nounVerbCountByLabel[\"punctuation\"] = punctCount + 1\r\n",
        "\r\n",
        "print(\"Positive\")\r\n",
        "print(\"noun\", nounVerbCountByLabel[\"positive\"][\"noun\"])\r\n",
        "print(\"verb\", nounVerbCountByLabel[\"positive\"][\"verb\"])\r\n",
        "print(\"Negative\")\r\n",
        "print(\"noun\", nounVerbCountByLabel[\"negative\"][\"noun\"])\r\n",
        "print(\"verb\", nounVerbCountByLabel[\"negative\"][\"verb\"])\r\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive\n",
            "noun 975924\n",
            "verb 540747\n",
            "Negative\n",
            "noun 916579\n",
            "verb 562823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJH_YRAHJ9p7"
      },
      "source": [
        "Question 8) (20 points) Using the results from the PoS process in question 7, count how many\r\n",
        "different PUNCT tags are found and their respective counts from the full dataset provided (both\r\n",
        "negative and positives together). Using regex, write a set of regular expressions that generate\r\n",
        "the same counts from the dataset without using NLTK or Spacy, just regex. Can you get the\r\n",
        "same counts? If not, why do you think this is?\r\n",
        "\r\n",
        "\r\n",
        "I did not receive the same counts, I belive that is because regex didn't take into consideration all possible variations of where a punctuation could be located. Such as I did not account for html characters while spacey did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r8tH6YAJ_Bx",
        "outputId": "d046b174-e024-4718-caa3-d394b5d4b441"
      },
      "source": [
        "\r\n",
        "print(\"Regex Punct Total: \", regexPunctCount)\r\n",
        "print(\"spacey punct count: \", nounVerbCountByLabel[\"punctuation\"])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regex Punct Total:  661014\n",
            "spacey punct count:  834967\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}