{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnMTBftTbgdEjEiinRDggn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nogbazghi/CSC8980/blob/main/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axt1FK7pCf1b"
      },
      "source": [
        "Nahom Ogbazghi\r\n",
        "002052292"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Iy12fauCbM_",
        "outputId": "6e57ca22-7c00-4e4a-f70a-ba75221adacc"
      },
      "source": [
        "!pip install conllu\r\n",
        "!wget http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\r\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (4.4)\n",
            "--2021-03-11 04:37:28--  http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/x-gzip]\n",
            "Saving to: ‘review_polarity.tar.gz.3’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  11.0MB/s    in 0.3s    \n",
            "\n",
            "2021-03-11 04:37:28 (11.0 MB/s) - ‘review_polarity.tar.gz.3’ saved [3127238/3127238]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5IME9dQ91W0"
      },
      "source": [
        "!tar zxf review_polarity.tar.gz"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T33uyO0EgCay",
        "outputId": "57cf835c-e3fe-4b9e-af1e-b068a8d3f081"
      },
      "source": [
        "import os\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk import word_tokenize\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0NPp2VQHl96"
      },
      "source": [
        "Using NLTK tokenize all documents, separated by polarity, remove stop words , and list the top 20 most frequent tokens (and their counts) for the positive reviews, and the top 20 most frequent tokens (and their counts). What kind of things do you notice are different between the two sets? (30 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSMoG4iuHqrO"
      },
      "source": [
        "I see very little differences. They have overwhelmingly similar similar words, except with different counts. For both, Punctuations were the majority of the largest count tokens as well as words like time, one, movie, like. There were differences: neg had tokens like: !, bad, get; while pos had words like character and story. Additionally some punctuations were more frequent in neg, compared to pos like ? and !."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB5pvjYbJdxB",
        "outputId": "de2e99d0-40fb-4f29-c30e-f97612d3e8e2"
      },
      "source": [
        "path = \"/content/txt_sentoken/\"\r\n",
        "posTokens = {}\r\n",
        "negTokens = {}\r\n",
        "polarity = [\"neg/\", \"pos/\"]\r\n",
        "for p in polarity:\r\n",
        "  for filename in os.listdir(path+p):\r\n",
        "    filepath = path+p+filename\r\n",
        "    file = open(filepath, 'r')\r\n",
        "    content = file.read()\r\n",
        "    word_tokens = word_tokenize(content)\r\n",
        "    if(p == \"neg/\"):\r\n",
        "      for w in word_tokens:  \r\n",
        "        if w not in stop_words:\r\n",
        "          if w in negTokens:\r\n",
        "            negCountForW = negTokens[w]\r\n",
        "            negTokens[w] = negCountForW + 1\r\n",
        "          else:\r\n",
        "            negTokens[w] = 1\r\n",
        "    else:\r\n",
        "      for w in word_tokens:  \r\n",
        "        if w not in stop_words:\r\n",
        "          if w in posTokens:\r\n",
        "            posCountForW = posTokens[w]\r\n",
        "            posTokens[w] = posCountForW + 1\r\n",
        "          else:\r\n",
        "            posTokens[w] = 1\r\n",
        "\r\n",
        "sortedNegCount = sorted(negTokens.items(), key=lambda x: x[1], reverse=True)\r\n",
        "print(\"Top 20 Tokens for Negative\")\r\n",
        "top_neg = sortedNegCount[:20]\r\n",
        "for w in top_neg:\r\n",
        "  print(w[0], \"-\", w[1])\r\n",
        "\r\n",
        "sortedPosCount = sorted(posTokens.items(), key=lambda x: x[1], reverse=True)\r\n",
        "print(\"Top 20 Tokens for Positive\")\r\n",
        "top_pos = sortedPosCount[:20]\r\n",
        "for w in top_pos:\r\n",
        "  print(w[0], \"-\", w[1])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 20 Tokens for Negative\n",
            ", - 35269\n",
            ". - 32162\n",
            "`` - 9123\n",
            "'s - 8655\n",
            ") - 5742\n",
            "( - 5650\n",
            "film - 4257\n",
            "n't - 3442\n",
            "movie - 3174\n",
            "one - 2637\n",
            "? - 2201\n",
            "like - 1832\n",
            ": - 1540\n",
            "even - 1381\n",
            "would - 1185\n",
            "good - 1126\n",
            "time - 1111\n",
            "! - 1056\n",
            "get - 1039\n",
            "bad - 1019\n",
            "Top 20 Tokens for Positive\n",
            ", - 42448\n",
            ". - 33714\n",
            "'s - 9473\n",
            "`` - 8494\n",
            ") - 6039\n",
            "( - 6014\n",
            "film - 5186\n",
            "one - 2943\n",
            "n't - 2775\n",
            "movie - 2497\n",
            "like - 1713\n",
            "? - 1570\n",
            ": - 1502\n",
            "story - 1231\n",
            "also - 1200\n",
            "good - 1190\n",
            "even - 1175\n",
            "time - 1171\n",
            "would - 1079\n",
            "character - 1067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KJ2bN-I6c8i",
        "outputId": "0b5d91b6-8293-414e-d11b-716cdbde243d"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "pos_df = []\r\n",
        "neg_df = []\r\n",
        "sanitized_pos_df = []\r\n",
        "sanitized_neg_df = []\r\n",
        "posC = 0\r\n",
        "negC = 0\r\n",
        "for p in polarity:\r\n",
        "  for filename in os.listdir(path+p):\r\n",
        "    filepath = path+p+filename\r\n",
        "    file = open(filepath, 'r')\r\n",
        "    content = file.read()\r\n",
        "    split_content = content.split()\r\n",
        "    if(p == \"pos/\"):\r\n",
        "      d = {\"polarity\": 1, \"text\": content}\r\n",
        "      df = pd.DataFrame(data=d, index=[posC])\r\n",
        "      pos_df.append(df)\r\n",
        "      filteredText = \"\"\r\n",
        "      for w in split_content:\r\n",
        "        if w not in stop_words:\r\n",
        "          filteredText = filteredText + \" \" + w\r\n",
        "      filteredD = {\"polarity\": 1, \"text\": filteredText}\r\n",
        "      filtered_df = pd.DataFrame(data=d, index=[posC])\r\n",
        "      sanitized_pos_df.append(filtered_df)\r\n",
        "      posC+=1\r\n",
        "    else:\r\n",
        "      d = {\"polarity\": 0, \"text\": content}\r\n",
        "      df = pd.DataFrame(data=d, index=[negC])\r\n",
        "      neg_df.append(df)\r\n",
        "      filteredText = \"\"\r\n",
        "      for w in split_content:\r\n",
        "        if w not in stop_words:\r\n",
        "          filteredText = filteredText + \" \" + w\r\n",
        "      filteredD = {\"polarity\": 1, \"text\": filteredText}\r\n",
        "      filtered_df = pd.DataFrame(data=d, index=[negC])\r\n",
        "      sanitized_neg_df.append(filtered_df)\r\n",
        "      negC+=1\r\n",
        "\r\n",
        "all_pos = pd.concat(pos_df)\r\n",
        "all_neg = pd.concat(neg_df)\r\n",
        "print(\"filtered collection:\")\r\n",
        "filtered_pos = pd.concat(sanitized_pos_df)\r\n",
        "filtered_neg = pd.concat(sanitized_neg_df)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filtered collection:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIFSi0IxF3X7"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSrd55cOF4L9"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\r\n",
        "pos_vectors = vectorizer.fit_transform(all_pos.text)\r\n",
        "pos_words_df = pd.DataFrame(pos_vectors.toarray(), columns=vectorizer.get_feature_names())\r\n",
        "pos_words_df.head()\r\n",
        "pos_X = pos_words_df\r\n",
        "pos_y = all_pos.polarity"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UHV4FaGMwg7"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\r\n",
        "neg_vectors = vectorizer.fit_transform(all_neg.text)\r\n",
        "neg_words_df = pd.DataFrame(neg_vectors.toarray(), columns=vectorizer.get_feature_names())\r\n",
        "neg_words_df.head()\r\n",
        "neg_X = neg_words_df\r\n",
        "neg_y = all_neg.polarity"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79Goe0wWMk0T"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# a 50% pos, 70 neg\r\n",
        "A_pos_X_train, A_pos_X_test, A_pos_y_train, A_pos_y_test = train_test_split(pos_X, pos_y, train_size=0.5)\r\n",
        "A_neg_X_train, A_neg_X_test, A_neg_y_train, A_neg_y_test = train_test_split(neg_X, neg_y, train_size=0.7)\r\n",
        "A_X_train = pd.concat([A_pos_X_train, A_neg_X_train])\r\n",
        "A_X_train = A_X_train.fillna(0)\r\n",
        "A_y_train = pd.concat([A_pos_y_train, A_neg_y_train])\r\n",
        "A_y_train = A_y_train.fillna(0)\r\n",
        "\r\n",
        "# b 70% pos, 50 neg\r\n",
        "B_pos_X_train, B_pos_X_test, B_pos_y_train, B_pos_y_test = train_test_split(pos_X, pos_y, train_size=0.7)\r\n",
        "B_neg_X_train, B_neg_X_test, B_neg_y_train, B_neg_y_test = train_test_split(neg_X, neg_y, train_size=0.5)\r\n",
        "B_X_train = pd.concat([B_pos_X_train, B_neg_X_train])\r\n",
        "B_X_train = B_X_train.fillna(0)\r\n",
        "B_y_train = pd.concat([B_pos_y_train, B_neg_y_train])\r\n",
        "B_y_train = B_y_train.fillna(0)\r\n",
        "\r\n",
        "# c 25% pos, 25 neg\r\n",
        "C_pos_X_train, C_pos_X_test, C_pos_y_train, C_pos_y_test = train_test_split(pos_X, pos_y, train_size=0.25)\r\n",
        "C_neg_X_train, C_neg_X_test, C_neg_y_train, C_neg_y_test = train_test_split(neg_X, neg_y, train_size=0.25)\r\n",
        "C_X_train = pd.concat([C_pos_X_train, C_neg_X_train])\r\n",
        "C_X_train = C_X_train.fillna(0)\r\n",
        "C_y_train = pd.concat([C_pos_y_train, C_neg_y_train])\r\n",
        "C_y_train = C_y_train.fillna(0)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YchqFylR56u"
      },
      "source": [
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk-mGxcvlomw",
        "outputId": "6daabbc1-d8bc-4b8d-99cd-ff434666448c"
      },
      "source": [
        "print(\"Train A NB\")\r\n",
        "# a\r\n",
        "# Create and train a multinomial naive bayes classifier (MultinomialNB)\r\n",
        "modelA = MultinomialNB()\r\n",
        "modelA.fit(A_X_train, A_y_train)\r\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train A NB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h8jmkj9lu9a",
        "outputId": "ecbcd848-8eaa-4662-b954-512aa41f056d"
      },
      "source": [
        "print(\"Train B NB\")\r\n",
        "# b\r\n",
        "# Create and train a multinomial naive bayes classifier (MultinomialNB)\r\n",
        "modelB = MultinomialNB()\r\n",
        "modelB.fit(B_X_train, B_y_train)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train B NB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wssrByTXlscP",
        "outputId": "3c1f7103-f3f1-4d45-cde6-60a136852fa0"
      },
      "source": [
        "print(\"Train C SVM\")\r\n",
        "# c\r\n",
        "# Create and train a linear support vector classifier (LinearSVC)\r\n",
        "modelC = LinearSVC()\r\n",
        "modelC.fit(C_X_train, C_y_train)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train C SVM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8__K5FxsmOQh"
      },
      "source": [
        "3. Using the models from question 2, evaluate them on their individual rest of the dataset. This is, for a) 50% positive and 30% negative, for b) 50% negative and 30% positive, and for c) 75% negative and 75% positive. Calculate and show ONLY the following metrics for each model: Accuracy, Precision, Recall, Macro F1-score. (15 points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWv6dgPWmSag"
      },
      "source": [
        "A_X_test = pd.concat([A_pos_X_test, A_neg_X_test])\r\n",
        "A_X_test = A_X_test.fillna(0)\r\n",
        "A_y_test = pd.concat([A_pos_y_test, A_neg_y_test])\r\n",
        "A_y_test = A_y_test.fillna(0)\r\n",
        "\r\n",
        "B_X_test = pd.concat([B_pos_X_test, B_neg_X_test])\r\n",
        "B_X_test = B_X_test.fillna(0)\r\n",
        "B_y_test = pd.concat([B_pos_y_test, B_neg_y_test])\r\n",
        "B_y_test = B_y_test.fillna(0)\r\n",
        "\r\n",
        "C_X_test = pd.concat([C_pos_X_test, C_neg_X_test])\r\n",
        "C_X_test = C_X_test.fillna(0)\r\n",
        "C_y_test = pd.concat([C_pos_y_test, C_neg_y_test])\r\n",
        "C_y_test = C_y_test.fillna(0)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btW4JEtoVeEH"
      },
      "source": [
        "def printScores(testData, labels):\r\n",
        "  print(\"Accuracy\", sklearn.metrics.accuracy_score(testData, labels)) \r\n",
        "  print(\"Precision\", sklearn.metrics.precision_score(testData, labels))\r\n",
        "  print(\"Recall\", sklearn.metrics.recall_score(testData, labels))\r\n",
        "  print(\"F1 Score\", sklearn.metrics.f1_score(labels, testData, average='macro'))"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FanKN2Oysm7R",
        "outputId": "2d3520e8-8770-4119-e1cb-bb88fb112c0f"
      },
      "source": [
        "import sklearn.metrics\r\n",
        "\r\n",
        "# A\r\n",
        "A_true = A_y_test\r\n",
        "A_labels = modelA.predict(A_X_test)\r\n",
        "print(\"A Scores\")\r\n",
        "printScores(A_true, A_labels)\r\n",
        "\r\n",
        "# B\r\n",
        "B_true = B_y_test\r\n",
        "B_labels = modelB.predict(B_X_test)\r\n",
        "print(\"\\nB Scores\")\r\n",
        "printScores(B_true, B_labels)\r\n",
        "\r\n",
        "# C\r\n",
        "C_true = C_y_test\r\n",
        "C_labels = modelC.predict(C_X_test)\r\n",
        "print(\"\\nC Scores\")\r\n",
        "printScores(C_true, C_labels)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Scores\n",
            "Accuracy 0.375\n",
            "Precision 0.0\n",
            "Recall 0.0\n",
            "F1 Score 0.2727272727272727\n",
            "\n",
            "B Scores\n",
            "Accuracy 0.375\n",
            "Precision 0.375\n",
            "Recall 1.0\n",
            "F1 Score 0.2727272727272727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "C Scores\n",
            "Accuracy 0.706\n",
            "Precision 0.710204081632653\n",
            "Recall 0.696\n",
            "F1 Score 0.7059705970597059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJhGwPtt2oYJ"
      },
      "source": [
        "Using the model performance metrics from question 3, answer the following questions. Please provide logical and intuitive rationale for your answers, simple answers like: because it has the best score, will not be sufficient. (40 points): a) What is the best performing model? b) Why do you think this is the best performing model? c) How does class imbalance play in determining polarity? d) Do you think either more data or a better model is a better approach for this kind of task?\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp0EaKn62syF"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   The best overall performing model was C. SVM, but A had the highest Precision, and B had the highest Recall.\r\n",
        "*   After researching SVM, I believed it was the best performing model because it finds a strong enough difference when the differences of the labeled data are often minimal, and looking at the text provided to us, the difference is minimum.\r\n",
        "*   Polarity is impacted because when one label provided more data than the other, there will be more unique words available to one label than the other.\r\n",
        "*   More data would only continue down the path of a badly balanced data set. A better model would be ideal if it will take into consideration the class imbalancing and leveling the imbalanced data to avoid mislabeling. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lme9E4A2RY6h",
        "outputId": "d71301c5-652b-42ae-a2f3-afa9be519253"
      },
      "source": [
        "nltk.download('vader_lexicon')\r\n",
        "nltk.download('movie_reviews')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5hNgUhvYEch"
      },
      "source": [
        "def polarityScore(docs):\r\n",
        "  texts = docs[\"text\"].values\r\n",
        "  pol = docs[\"polarity\"].values\r\n",
        "  neut = 0\r\n",
        "  neg = 0\r\n",
        "  pos = 0\r\n",
        "  comp = 0\r\n",
        "  \r\n",
        "  for text in texts:\r\n",
        "    score = sia.polarity_scores(text)\r\n",
        "    neut += score[\"neu\"]\r\n",
        "    neg += score[\"neg\"]\r\n",
        "    pos += score[\"pos\"]\r\n",
        "    comp += score[\"compound\"]\r\n",
        "  neut = neut/len(texts)\r\n",
        "  neg = neg/len(texts)\r\n",
        "  pos = pos/len(texts)\r\n",
        "  comp = comp/len(texts)\r\n",
        "  return neut, neg, pos, comp"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYHwpV9mS2FI",
        "outputId": "6e6c1248-3f28-4d33-d2c2-23b8389ab1b8"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\r\n",
        "\r\n",
        "sia = SIA()\r\n",
        "p_neu, p_neg, p_pos, p_comp = polarityScore(all_pos)\r\n",
        "print(\"Average polarity positive scores:\")\r\n",
        "print(\"neutral:\",p_neu,\"negative:\", p_neg, \"positive:\", p_pos, \"compound:\", p_comp)\r\n",
        "\r\n",
        "n_neu, n_neg, n_pos, n_comp = polarityScore(all_neg)\r\n",
        "print(\"Average negative positive scores:\")\r\n",
        "print(\"neutral:\",n_neu, \"negative:\", n_neg, \"positive:\", n_pos, \"compound:\", n_comp)\r\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average polarity positive scores:\n",
            "neutral: 0.7596550000000001 negative: 0.09025199999999997 positive: 0.15009199999999998 compound: 0.6479031999999999\n",
            "Average negative positive scores:\n",
            "neutral: 0.7631079999999995 negative: 0.11317100000000005 positive: 0.12373599999999989 compound: 0.10536420000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-1OUrSxd3qV"
      },
      "source": [
        "Using NLTK and VADER, calculate the sentiment score for all documents in the positive polarity. Calculate the polarity threshold needed (and reasonable) to have the majority of the document labels match. Do the same for the negative class. Provide the threshold needed, the reason why you think this threshold is reasonable, and the accuracy percentage (how many documents are correctly labeled using this threshold). (45 points):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zey3oe69eYiv"
      },
      "source": [
        "The threshold is the average compound: all compounds divided by the amount of text documents. Additionally, to widen the potential matches the Postive average compound is reduced, as well as the negative average compound score is increased, both by a minimal percentage. For positive, it is increased by 6%, while negative is reduced by 3%. Additional to the compound, I used the negative and positive scores to improve the negative labeling score.\r\n",
        "I believe these thresholds are reasonable because for compound there is a steep difference between the positive and negative scores. This allowed me to gain the best return. I would have preferred a better true negative score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_sfLJyBqhPM",
        "outputId": "19de4bc8-4ade-406c-cb60-fcbe574e2eb8"
      },
      "source": [
        "def plusPercentDeviation(num):\r\n",
        "  return num + (num * 0.06)\r\n",
        "\r\n",
        "def minusPercentDeviation(num):\r\n",
        "  return num - (num * 0.03)\r\n",
        "\r\n",
        "test_p_neu = plusPercentDeviation(p_neu)\r\n",
        "test_p_neg = plusPercentDeviation(p_neg)\r\n",
        "test_p_pos = minusPercentDeviation(p_pos)\r\n",
        "test_p_comp = minusPercentDeviation(p_comp)\r\n",
        "test_n_neu = plusPercentDeviation(n_neu)\r\n",
        "test_n_neg = minusPercentDeviation(n_neg)\r\n",
        "test_n_pos = plusPercentDeviation(n_pos)\r\n",
        "test_n_comp = plusPercentDeviation(n_comp)\r\n",
        "countP = 0\r\n",
        "countN = 0\r\n",
        "countFN = 0\r\n",
        "countFP = 0\r\n",
        "\r\n",
        "for text in all_pos[\"text\"].values:\r\n",
        "  score = sia.polarity_scores(text)\r\n",
        "  neut = score[\"neu\"]\r\n",
        "  neg = score[\"neg\"]\r\n",
        "  pos = score[\"pos\"]\r\n",
        "  comp = score[\"compound\"]\r\n",
        "  if(comp > test_p_comp):\r\n",
        "    countP += 1\r\n",
        "  elif(pos > test_p_pos):\r\n",
        "    countP += 1\r\n",
        "  else:\r\n",
        "    countFN += 1\r\n",
        "\r\n",
        "for text in all_neg[\"text\"].values:\r\n",
        "  score = sia.polarity_scores(text)\r\n",
        "  neut = score[\"neu\"]\r\n",
        "  neg = score[\"neg\"]\r\n",
        "  pos = score[\"pos\"]\r\n",
        "  comp = score[\"compound\"]\r\n",
        "  if(comp < test_n_comp):\r\n",
        "    countN += 1\r\n",
        "  elif(neg > test_n_neg):\r\n",
        "    countN += 1\r\n",
        "  elif(pos < test_n_pos):\r\n",
        "    countN += 1\r\n",
        "  else:\r\n",
        "    countFP += 1\r\n",
        "\r\n",
        "print(\"False Positive\")\r\n",
        "print((countFP/(len(all_neg[\"text\"].values)))*100)\r\n",
        "print(\"False Negative\")\r\n",
        "print((countFN/(len(all_pos[\"text\"].values)))*100)\r\n",
        "print(\"True Negative\")\r\n",
        "print((countN/(len(all_neg[\"text\"].values)))*100)\r\n",
        "print(\"True Positives\")\r\n",
        "print((countP/(len(all_pos[\"text\"].values)))*100)\r\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False Positive\n",
            "22.3\n",
            "False Negative\n",
            "17.599999999999998\n",
            "True Negative\n",
            "77.7\n",
            "True Positives\n",
            "82.39999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6Llp66Msz57"
      },
      "source": [
        "Bonus (40 points): Repeat questions 2,3 and 4 removing all stopwords. Answer the following questions: Did this change the results in any way? Why do you think so?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld8iAcKNYiZH"
      },
      "source": [
        "It did not change the result, I believe that is due to the fact the stop words were irrelevant in terms of value and they were neither unique to either positive or negative texts and thus not impactful to the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89I1dl0qThhA"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyCB7Y7kThhC",
        "outputId": "dea3b3d0-a560-4973-f355-2c9ebaea8e0b"
      },
      "source": [
        "print(\"Filtered:\")\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "pos_vectors = vectorizer.fit_transform(filtered_pos.text)\r\n",
        "pos_words_df = pd.DataFrame(pos_vectors.toarray(), columns=vectorizer.get_feature_names())\r\n",
        "pos_words_df.head()\r\n",
        "pos_X = pos_words_df\r\n",
        "pos_y = filtered_pos.polarity"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ1dAU8tThhD"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\r\n",
        "neg_vectors = vectorizer.fit_transform(filtered_neg.text)\r\n",
        "neg_words_df = pd.DataFrame(neg_vectors.toarray(), columns=vectorizer.get_feature_names())\r\n",
        "neg_words_df.head()\r\n",
        "neg_X = neg_words_df\r\n",
        "neg_y = filtered_neg.polarity"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aviQ-NZpTGPn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# a 50% pos, 70 neg\r\n",
        "A_pos_X_train, A_pos_X_test, A_pos_y_train, A_pos_y_test = train_test_split(pos_X, pos_y, train_size=0.5)\r\n",
        "A_neg_X_train, A_neg_X_test, A_neg_y_train, A_neg_y_test = train_test_split(neg_X, neg_y, train_size=0.7)\r\n",
        "A_X_train = pd.concat([A_pos_X_train, A_neg_X_train])\r\n",
        "A_X_train = A_X_train.fillna(0)\r\n",
        "A_y_train = pd.concat([A_pos_y_train, A_neg_y_train])\r\n",
        "A_y_train = A_y_train.fillna(0)\r\n",
        "\r\n",
        "# b 70% pos, 50 neg\r\n",
        "B_pos_X_train, B_pos_X_test, B_pos_y_train, B_pos_y_test = train_test_split(pos_X, pos_y, train_size=0.7)\r\n",
        "B_neg_X_train, B_neg_X_test, B_neg_y_train, B_neg_y_test = train_test_split(neg_X, neg_y, train_size=0.5)\r\n",
        "B_X_train = pd.concat([B_pos_X_train, B_neg_X_train])\r\n",
        "B_X_train = B_X_train.fillna(0)\r\n",
        "B_y_train = pd.concat([B_pos_y_train, B_neg_y_train])\r\n",
        "B_y_train = B_y_train.fillna(0)\r\n",
        "\r\n",
        "# c 25% pos, 25 neg\r\n",
        "C_pos_X_train, C_pos_X_test, C_pos_y_train, C_pos_y_test = train_test_split(pos_X, pos_y, train_size=0.25)\r\n",
        "C_neg_X_train, C_neg_X_test, C_neg_y_train, C_neg_y_test = train_test_split(neg_X, neg_y, train_size=0.25)\r\n",
        "C_X_train = pd.concat([C_pos_X_train, C_neg_X_train])\r\n",
        "C_X_train = C_X_train.fillna(0)\r\n",
        "C_y_train = pd.concat([C_pos_y_train, C_neg_y_train])\r\n",
        "C_y_train = C_y_train.fillna(0)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0Y2y04BTPrg",
        "outputId": "8deae884-9e93-4e9e-fec7-40fdbb1d9d5a"
      },
      "source": [
        "print(\"Train A NB\")\r\n",
        "# a\r\n",
        "# Create and train a multinomial naive bayes classifier (MultinomialNB)\r\n",
        "filtmodelA = MultinomialNB()\r\n",
        "filtmodelA.fit(A_X_train, A_y_train)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train A NB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65p6lSk2TPrt",
        "outputId": "544c905d-8ff8-485a-a258-0c93356e1d45"
      },
      "source": [
        "print(\"Train B NB\")\r\n",
        "# b\r\n",
        "# Create and train a multinomial naive bayes classifier (MultinomialNB)\r\n",
        "filtmodelB = MultinomialNB()\r\n",
        "filtmodelB.fit(B_X_train, B_y_train)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train B NB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJj8tIhOTPru",
        "outputId": "ba24ee9e-25c5-4a43-b312-ef923b54ef22"
      },
      "source": [
        "print(\"Train C SVM\")\r\n",
        "# c\r\n",
        "# Create and train a linear support vector classifier (LinearSVC)\r\n",
        "filtmodelC = LinearSVC()\r\n",
        "filtmodelC.fit(C_X_train, C_y_train)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train C SVM\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfYwyXB_UETj"
      },
      "source": [
        "A_X_test = pd.concat([A_pos_X_test, A_neg_X_test])\r\n",
        "A_X_test = A_X_test.fillna(0)\r\n",
        "A_y_test = pd.concat([A_pos_y_test, A_neg_y_test])\r\n",
        "A_y_test = A_y_test.fillna(0)\r\n",
        "\r\n",
        "B_X_test = pd.concat([B_pos_X_test, B_neg_X_test])\r\n",
        "B_X_test = B_X_test.fillna(0)\r\n",
        "B_y_test = pd.concat([B_pos_y_test, B_neg_y_test])\r\n",
        "B_y_test = B_y_test.fillna(0)\r\n",
        "\r\n",
        "C_X_test = pd.concat([C_pos_X_test, C_neg_X_test])\r\n",
        "C_X_test = C_X_test.fillna(0)\r\n",
        "C_y_test = pd.concat([C_pos_y_test, C_neg_y_test])\r\n",
        "C_y_test = C_y_test.fillna(0)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYOy9Ja_Uc5D"
      },
      "source": [
        "def printScores(testData, labels):\r\n",
        "  print(\"Accuracy\", sklearn.metrics.accuracy_score(testData, labels)) \r\n",
        "  print(\"Precision\", sklearn.metrics.precision_score(testData, labels))\r\n",
        "  print(\"Recall\", sklearn.metrics.recall_score(testData, labels))\r\n",
        "  print(\"F1 Score\", sklearn.metrics.f1_score(labels, testData, average='macro'))"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZG9XYSGUETj",
        "outputId": "425ad50e-b85d-49a9-f612-230c9e53fec3"
      },
      "source": [
        "import sklearn.metrics\r\n",
        "\r\n",
        "# A\r\n",
        "A_true = A_y_test\r\n",
        "A_labels = filtmodelA.predict(A_X_test)\r\n",
        "print(\"A Scores\")\r\n",
        "printScores(A_true, A_labels)\r\n",
        "\r\n",
        "# B\r\n",
        "B_true = B_y_test\r\n",
        "B_labels = filtmodelB.predict(B_X_test)\r\n",
        "print(\"\\nB Scores\")\r\n",
        "printScores(B_true, B_labels)\r\n",
        "\r\n",
        "# C\r\n",
        "C_true = C_y_test\r\n",
        "C_labels = filtmodelC.predict(C_X_test)\r\n",
        "print(\"\\nC Scores\")\r\n",
        "printScores(C_true, C_labels)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A Scores\n",
            "Accuracy 0.375\n",
            "Precision 0.0\n",
            "Recall 0.0\n",
            "F1 Score 0.2727272727272727\n",
            "\n",
            "B Scores\n",
            "Accuracy 0.375\n",
            "Precision 0.375\n",
            "Recall 1.0\n",
            "F1 Score 0.2727272727272727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "C Scores\n",
            "Accuracy 0.7213333333333334\n",
            "Precision 0.7106598984771574\n",
            "Recall 0.7466666666666667\n",
            "F1 Score 0.7211543764086819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EVxS6fZW_FO"
      },
      "source": [
        "def polarityScore(docs):\r\n",
        "  texts = docs[\"text\"].values\r\n",
        "  pol = docs[\"polarity\"].values\r\n",
        "  neut = 0\r\n",
        "  neg = 0\r\n",
        "  pos = 0\r\n",
        "  comp = 0\r\n",
        "  \r\n",
        "  for text in texts:\r\n",
        "    score = sia.polarity_scores(text)\r\n",
        "    neut += score[\"neu\"]\r\n",
        "    neg += score[\"neg\"]\r\n",
        "    pos += score[\"pos\"]\r\n",
        "    comp += score[\"compound\"]\r\n",
        "  neut = neut/len(texts)\r\n",
        "  neg = neg/len(texts)\r\n",
        "  pos = pos/len(texts)\r\n",
        "  comp = comp/len(texts)\r\n",
        "  return neut, neg, pos, comp"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA_TyPRQW_Fe",
        "outputId": "c19605dc-44a0-4861-ca66-73acef232745"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\r\n",
        "\r\n",
        "sia = SIA()\r\n",
        "p_neu, p_neg, p_pos, p_comp = polarityScore(filtered_pos)\r\n",
        "print(\"Filtered\")\r\n",
        "print(\"Average polarity positive scores:\")\r\n",
        "print(\"neutral:\",p_neu,\"negative:\", p_neg, \"positive:\", p_pos, \"compound:\", p_comp)\r\n",
        "\r\n",
        "n_neu, n_neg, n_pos, n_comp = polarityScore(filtered_neg)\r\n",
        "print(\"Average negative positive scores:\")\r\n",
        "print(\"neutral:\",n_neu, \"negative:\", n_neg, \"positive:\", n_pos, \"compound:\", n_comp)\r\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered\n",
            "Average polarity positive scores:\n",
            "neutral: 0.7596550000000001 negative: 0.09025199999999997 positive: 0.15009199999999998 compound: 0.6479031999999999\n",
            "Average negative positive scores:\n",
            "neutral: 0.7631079999999995 negative: 0.11317100000000005 positive: 0.12373599999999989 compound: 0.10536420000000002\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}